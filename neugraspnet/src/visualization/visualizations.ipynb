{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "# import cv2\n",
    "import open3d as o3d\n",
    "from open3d import JVisualizer\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "# import scipy.signal as signal\n",
    "# from tqdm import tqdm\n",
    "# import multiprocessing as mp\n",
    "\n",
    "# from neugraspnet.grasp import Grasp, Label\n",
    "from neugraspnet.io import *\n",
    "from neugraspnet.perception import *\n",
    "from neugraspnet.simulation import ClutterRemovalSim\n",
    "from neugraspnet.utils.transform import Rotation, Transform\n",
    "from neugraspnet.utils.implicit import get_scene_from_mesh_pose_list, as_mesh\n",
    "from neugraspnet.utils.misc import apply_noise\n",
    "from neugraspnet.grasp_sampler import GpgGraspSamplerPcl\n",
    "from neugraspnet.networks import get_network, load_network\n",
    "\n",
    "# seed = np.random.randint(2**32 - 1)\n",
    "nice_seeds = [2744579596, 3736952697, 1570237463, 3573398670, 3090687052]\n",
    "great_scene_forscene_recon = [2837027833, 1075486976]\n",
    "seed = nice_seeds[4]\n",
    "np.random.seed(seed)\n",
    "\n",
    "constructed_root = Path(\"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/pile/data_pile_train_constructed_4M_HighRes_radomized_views_GPG_only\")\n",
    "model_type = \"neu_grasp_pn_deeper\"\n",
    "# model_path = \"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/runs_relevant/23-05-07-10-46-43_dataset=data_packed_train_constructed_4M_GPG_60_randomized_view_no_tab_packked,augment=False,net=6d_neu_grasp_pn_deeper,batch_size=32,lr=5e-05,PN_deeper_no_tab_WITH_occ_PACKED/best_neural_grasp_neu_grasp_pn_deeper_val_acc=0.9317.pt\"\n",
    "# model_path=\"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/runs_relevant/23-05-03-01-42-37_dataset=data_pile_train_constructed_4M_HighRes_radomized_views_no_table,augment=False,net=6d_neu_grasp_pn_deeper,batch_size=32,lr=5e-05,PN_no_tab_deeper_DIMS_WITH_occ/best_neural_grasp_neu_grasp_pn_deeper_val_acc=0.9120.pt\"\n",
    "model_path = \"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/runs_relevant/23-05-01-08-11-39_dataset=data_pile_train_constructed_4M_HighRes_radomized_views,augment=False,net=6d_neu_grasp_pn_deeper,batch_size=32,lr=5e-05,PN_deeper_DIMS_CONT/best_neural_grasp_neu_grasp_pn_deeper_val_acc=0.9097.pt\"\n",
    "see_table = True\n",
    "device = \"cuda\"\n",
    "net = load_network(model_path, device, model_type)\n",
    "\n",
    "# previous_root = \"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/packed/packed_data_for_pngpd\"\n",
    "previous_root=\"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/pile/data_pile_train_random_raw_4M_radomized_views/\"\n",
    "data_root = \"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/\"\n",
    "\n",
    "sim_gui = False\n",
    "three_cameras = True # Use one camera for wrist and two cameras for the fingers\n",
    "add_noise = False # Add dex noise to the rendered images like GIGA\n",
    "noise_type = 'mod_dex'\n",
    "gp_rate = 0.5 # Rate of applying Gaussian process noise\n",
    "voxel_downsample_size = 0.002 # 2mm\n",
    "scene_voxel_downsample_size = 0.005 # 5mm\n",
    "max_points = 1023\n",
    "resolution = 64\n",
    "scene='pile'\n",
    "object_set='pile/train'\n",
    "size=0.3\n",
    "\n",
    "## Re-create the saved simulation\n",
    "# Get random scene\n",
    "index = np.random.randint(32000) # index 20 is a good example.\n",
    "mesh_list_files = glob.glob(os.path.join(previous_root, 'mesh_pose_list', '*.npz'))\n",
    "mesh_pose_list = np.load(mesh_list_files[index], allow_pickle=True)['pc']\n",
    "scene_id = os.path.basename(mesh_list_files[index])[:-4] # scene id without .npz extension\n",
    "## Get specific scene\n",
    "# scene_id = 'f614e39ed9df4e1094d569cddc20979b'\n",
    "# mesh_list_file = os.path.join(previous_root, 'mesh_pose_list', scene_id + '.npz')\n",
    "# mesh_pose_list = np.load(mesh_list_file, allow_pickle=True)['pc']\n",
    "\n",
    "sim = ClutterRemovalSim(scene, object_set, gui=sim_gui, data_root=data_root) # parameters scene and object_set are not used\n",
    "sim.setup_sim_scene_from_mesh_pose_list(mesh_pose_list, table=see_table, data_root=data_root) # Setting table to False because we don't want to render it\n",
    "# sim.save_state()\n",
    "\n",
    "# Get scene point cloud and normals using ground truth meshes\n",
    "scene_mesh = get_scene_from_mesh_pose_list(mesh_pose_list, data_root=data_root)\n",
    "o3d_scene_mesh = scene_mesh.as_open3d\n",
    "o3d_scene_mesh.compute_vertex_normals()\n",
    "pc = o3d_scene_mesh.sample_points_uniformly(number_of_points=1000)\n",
    "points = np.asarray(pc.points)\n",
    "# pc_trimesh = trimesh.points.PointCloud(points)\n",
    "# pc_colors = np.array([trimesh.visual.random_color() for i in points])\n",
    "# pc_trimesh.vertices_color = pc_colors\n",
    "# trimesh.Scene([scene_mesh, pc_trimesh]).show()\n",
    "# o3d.visualization.draw_geometries([pc])\n",
    "visualizer = JVisualizer()\n",
    "# pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0, 0]), (np.asarray(pc.points).shape[0], 1)))\n",
    "# visualizer.add_geometry(pc)\n",
    "# visualizer.show()\n",
    "\n",
    "trimesh.Scene([scene_mesh]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render depth image from random viewpoint or hard viewpoint\n",
    "def render_n_images(sim, n=1, random=False, tight=False, noise_type=''):\n",
    "    origin = Transform(Rotation.identity(), np.r_[size / 2, size / 2, 0.0])\n",
    "    if random:\n",
    "        if tight:\n",
    "            theta = np.random.uniform(5.5*np.pi/12.0)\n",
    "        theta = np.random.uniform(np.pi / 12.0, 5* np.pi / 12.0) # elevation: 15 to 75 degrees\n",
    "\n",
    "        # theta = np.random.uniform(5*np.pi/12.0)\n",
    "        # 75 degree reconstruction is unrealistic, try 60\n",
    "        # theta = np.random.uniform(np.pi/3)\n",
    "        # theta = np.random.uniform(np.pi/6, np.pi/4) # elevation: 30 to 45 degrees\n",
    "        r = np.random.uniform(2.0) * size\n",
    "    else:\n",
    "        # theta = np.random.uniform(np.pi / 18.0, np.pi / 4.5) # SJ EDIT!!! random 10 to 40 degrees from top view\n",
    "        theta = np.pi / 4.0 # 45 degrees from top view\n",
    "        r = 2.0 * size\n",
    "    \n",
    "    phi_list = 2.0 * np.pi * np.arange(n) / n # circle around the scene\n",
    "    extrinsics = [camera_on_sphere(origin, r, theta, phi) for phi in phi_list]\n",
    "    depth_imgs = []\n",
    "\n",
    "    for extrinsic in extrinsics:\n",
    "        # Multiple views -> for getting other sides of pc\n",
    "        depth_img = sim.camera.render(extrinsic)[1]\n",
    "        # add noise\n",
    "        depth_img = apply_noise(depth_img, noise_type)\n",
    "        \n",
    "        depth_imgs.append(depth_img)\n",
    "\n",
    "    return depth_imgs, extrinsics\n",
    "\n",
    "# Get random scene image:\n",
    "depth_imgs, extrinsics = render_n_images(sim, n=1, random=True, tight=True, noise_type='')\n",
    "plt.imshow(depth_imgs[0]) # Normally too far away to see anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tsdf and pc from the image\n",
    "tsdf = TSDFVolume(size, resolution)\n",
    "for depth_img, extrinsic in zip(depth_imgs, extrinsics):\n",
    "    tsdf.integrate(depth_img, sim.camera.intrinsic, extrinsic)\n",
    "seen_pc = tsdf.get_cloud()\n",
    "# Optional: Crop out table\n",
    "lower = np.array([0.0 , 0.0 , 0.055])\n",
    "upper = np.array([size, size, size])\n",
    "bounding_box = o3d.geometry.AxisAlignedBoundingBox(lower, upper)\n",
    "seen_pc = seen_pc.crop(bounding_box)\n",
    "# Optional: Downsample\n",
    "seen_pc = seen_pc.voxel_down_sample(scene_voxel_downsample_size)\n",
    "\n",
    "# Viz seen point cloud and camera position\n",
    "# seen_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0.64, 0.93]), (np.asarray(seen_pc.points).shape[0], 1)))\n",
    "cam_pos_pc = o3d.geometry.PointCloud()\n",
    "cam_pos_pc.points = o3d.utility.Vector3dVector(np.array([extrinsics[0].inverse().translation]))\n",
    "cam_pos_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0.64, 0.93]), (np.asarray(cam_pos_pc.points).shape[0], 1)))\n",
    "visualizer.add_geometry(seen_pc)\n",
    "# visualizer.add_geometry(cam_pos_pc)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_proposal(ray0, ray_direction, n_steps=256, depth_range=[0.001, 0.18]):\n",
    "    \n",
    "    d_proposal = torch.linspace(0, 1, steps=n_steps).view(1, 1, n_steps, 1)\n",
    "    d_proposal = depth_range[0] * (1. - d_proposal) + depth_range[1]* d_proposal\n",
    "\n",
    "    p_proposal = ray0.unsqueeze(2).repeat(1, 1, n_steps, 1) + ray_direction.unsqueeze(2).repeat(1, 1, n_steps, 1) * d_proposal\n",
    "\n",
    "    return p_proposal, d_proposal\n",
    "\n",
    "def secant(net, tsdf, f_low, f_high, d_low, d_high, n_secant_steps,\n",
    "                          ray0_masked, ray_direction_masked, tau):\n",
    "    ''' Runs the secant method for interval [d_low, d_high].\n",
    "\n",
    "    Args:\n",
    "        d_low (tensor): start values for the interval\n",
    "        d_high (tensor): end values for the interval\n",
    "        n_secant_steps (int): number of steps\n",
    "        ray0_masked (tensor): masked ray start points\n",
    "        ray_direction_masked (tensor): masked ray direction vectors\n",
    "        model (nn.Module): model model to evaluate point occupancies\n",
    "        c (tensor): latent conditioned code c\n",
    "        tau (float): threshold value in logits\n",
    "    '''\n",
    "    d_pred = - f_low * (d_high - d_low) / (f_high - f_low) + d_low\n",
    "    for i in range(n_secant_steps):\n",
    "        p_mid = ray0_masked + d_pred.unsqueeze(-1) * ray_direction_masked\n",
    "        p_scaled_mid = (p_mid/0.3 - 0.5).to(tsdf.device).float()\n",
    "        with torch.no_grad():\n",
    "            f_mid = (net.infer_occ(p_scaled_mid.view(1, -1, 3),  tsdf)- tau).squeeze()#.cpu()\n",
    "        ind_low = f_mid < 0\n",
    "        ind_low = ind_low\n",
    "        if ind_low.sum() > 0:\n",
    "            d_low[ind_low] = d_pred[ind_low]\n",
    "            f_low[ind_low] = f_mid[ind_low]\n",
    "        if (ind_low == 0).sum() > 0:\n",
    "            d_high[ind_low == 0] = d_pred[ind_low == 0]\n",
    "            f_high[ind_low == 0] = f_mid[ind_low == 0]\n",
    "\n",
    "        d_pred = - f_low * (d_high - d_low) / (f_high - f_low) + d_low\n",
    "    return d_pred\n",
    "\n",
    "def render_occ(intrinsic, extrinsics, net, input_tsdf, min_measured_depth, max_measured_depth, n_steps=256):\n",
    "\n",
    "    width, height = (intrinsic.K[:2, 2]*2).astype(int)\n",
    "    batch_size = n_images = 1 # Use 1 for now because GPU memory len(extrinsics)\n",
    "    n_pts = width*height\n",
    "\n",
    "    ## Generate proposal points\n",
    "\n",
    "    # Make pixel points\n",
    "    pixel_grid = torch.meshgrid(torch.arange(width), torch.arange(height))\n",
    "    pixels = torch.dstack((pixel_grid[0],pixel_grid[1])).reshape(-1, 2)\n",
    "    pixels_hom = torch.hstack((pixels, torch.ones((pixels.shape[0], 2)))) # Homogenous co-ordinates\n",
    "\n",
    "    cam_inv_extrinsics = torch.ones((batch_size, 4, 4), dtype=torch.float32)\n",
    "    camera_world = torch.zeros((batch_size, n_pts, 3))\n",
    "    # Using only first camera for now (GPU memory)\n",
    "    # for idx, cam_extrinsic in enumerate(extrinsics):\n",
    "    camera_world[0, :] = torch.from_numpy(extrinsics[0].inverse().translation).float()\n",
    "    cam_inv_extrinsics[0] = torch.from_numpy(extrinsics[0].inverse().as_matrix()).float()\n",
    "\n",
    "    # Convert pixel points to world co-ords:\n",
    "    intrinsic_hom = torch.eye(4)\n",
    "    intrinsic_hom[:3,:3] = torch.tensor(intrinsic.K)\n",
    "    image_plane_depth = 0.01 # 10mm\n",
    "    pixels_hom[:,:3] *= image_plane_depth # Multiply by depth\n",
    "    pixels_local_hom = (torch.inverse(intrinsic_hom) @ pixels_hom.T).unsqueeze(0).repeat(batch_size,1,1)\n",
    "    pixels_world = torch.bmm(cam_inv_extrinsics, pixels_local_hom)[:,:3,:].transpose(1,2)\n",
    "\n",
    "    ray_vector_world = (pixels_world - camera_world)\n",
    "    ray_vector_world = ray_vector_world/ray_vector_world.norm(2,2).unsqueeze(-1)\n",
    "\n",
    "    p_proposal_world, d_proposal = get_simple_proposal(camera_world,\n",
    "                                                    ray_vector_world,\n",
    "                                                    n_steps=n_steps,\n",
    "                                                    depth_range=[min_measured_depth, max_measured_depth])\n",
    "    # Debug: Viz proposal points\n",
    "    # prop_o3d = o3d.geometry.PointCloud()\n",
    "    # max_points = 2000\n",
    "    # points = p_proposal_world[0].view(-1,3)\n",
    "    # indices = np.random.randint(points.shape[0], size=max_points)\n",
    "    # prop_o3d.points = o3d.utility.Vector3dVector(points[indices].numpy())\n",
    "    # prop_o3d.colors = o3d.utility.Vector3dVector(np.random.uniform(0,1,size=(max_points,3)))\n",
    "    # visualizer3 = JVisualizer()\n",
    "    # visualizer3.add_geometry(prop_o3d)\n",
    "    # visualizer3.add_geometry(pc_full)\n",
    "    # visualizer3.show()\n",
    "\n",
    "    # Normalize and convert to query for network\n",
    "    p_scaled_proposal_query = ((p_proposal_world)/size - 0.5).view(batch_size,-1, 3)\n",
    "    # Query network\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        val = net.infer_occ(p_scaled_proposal_query.clone().to(device), input_tsdf)\n",
    "    val = val.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    val = (val - 0.5) # Center occupancies around 0\n",
    "\n",
    "    # Debug: Viz occupancied points\n",
    "    # points_occ = p_proposal_world.view(-1, 3)[val.squeeze()>0]\n",
    "    # occ_pc = o3d.geometry.PointCloud()\n",
    "    # occ_pc.points = o3d.utility.Vector3dVector(points_occ)\n",
    "    # occ_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.6, 0.0, 1]), (np.asarray(occ_pc.points).shape[0], 1)))\n",
    "    # visualizer4 = JVisualizer()\n",
    "    # visualizer4.add_geometry(occ_pc)\n",
    "    # visualizer4.add_geometry(pc_full)\n",
    "    # visualizer4.show()\n",
    "\n",
    "    ## Surface rendering\n",
    "    val = val.reshape(batch_size, -1, n_steps)\n",
    "    mask_0_not_occupied = val[:, :, 0] < 0\n",
    "    sign_matrix = torch.cat([torch.sign(val[:, :, :-1] * val[:, :, 1:]),\n",
    "                                    torch.ones(batch_size, n_pts, 1)],\n",
    "                                    dim=-1)\n",
    "    cost_matrix = sign_matrix * torch.arange(n_steps, 0, -1).float()\n",
    "    values, indices = torch.min(cost_matrix, -1)\n",
    "    mask_sign_change = values < 0\n",
    "    mask_neg_to_pos = val[torch.arange(batch_size).unsqueeze(-1),torch.arange(n_pts).unsqueeze(-0), indices] < 0\n",
    "    mask = mask_sign_change & mask_neg_to_pos & mask_0_not_occupied\n",
    "\n",
    "    n = batch_size * n_pts\n",
    "    d_proposal = d_proposal.repeat(1,n_pts,1,1)\n",
    "    d_low = d_proposal.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]\n",
    "    f_low = val.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(\n",
    "                batch_size, n_pts)[mask]\n",
    "    indices = torch.clamp(indices + 1, max=n_steps-1)\n",
    "    d_high = d_proposal.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]\n",
    "    f_high = val.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]\n",
    "\n",
    "    ray0_masked = camera_world[mask]\n",
    "    ray_direction_masked = ray_vector_world[mask]\n",
    "\n",
    "    # Apply surface depth refinement step (e.g. Secant method)\n",
    "    n_secant_steps = 8\n",
    "    with torch.no_grad():\n",
    "        d_pred = secant(net, input_tsdf,\n",
    "            f_low.cuda(), f_high.cuda(), d_low.cuda(), d_high.cuda(), n_secant_steps, ray0_masked.cuda(),\n",
    "            ray_direction_masked.cuda(), tau=torch.tensor(0.5, device=device))\n",
    "    d_pred = d_pred.cpu()\n",
    "    points_out = torch.zeros(batch_size, n_pts, 3, dtype=torch.float)#*t_nan # set default (invalid) values\n",
    "    points_out[mask] = ray0_masked + ray_direction_masked * d_pred.unsqueeze(1)\n",
    "\n",
    "    return points_out, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Render the scene using the occupancy network with the same extrinsics\n",
    "\n",
    "# Get views from a circular path around the scene\n",
    "depth_imgs_full, extrinsics_full = render_n_images(sim, n=6, random=False, noise_type='')\n",
    "\n",
    "# Neural render camera settings\n",
    "width = 64\n",
    "height = 64\n",
    "width_fov  = np.deg2rad(75) # angular FOV (120 by default)\n",
    "height_fov = np.deg2rad(75) # angular FOV (120 by default)\n",
    "f_x = width  / (np.tan(width_fov / 2.0))\n",
    "f_y = height / (np.tan(height_fov / 2.0))\n",
    "intrinsic = CameraIntrinsic(width, height, f_x, f_y, width/2, height/2)\n",
    "\n",
    "min_measured_depth = size\n",
    "max_measured_depth = 2.4*size + size/2 # max distance from the origin\n",
    "# max_measured_depth *= 0.75 # SJ_Edit!\n",
    "\n",
    "tsdf_t = torch.tensor(tsdf.get_grid(), device=device, dtype=torch.float32)\n",
    "\n",
    "surface_points_combined = None\n",
    "for cam_extrinsic in extrinsics_full:\n",
    "    surf_points_world, surf_mask = render_occ(intrinsic, [cam_extrinsic], net, tsdf_t, min_measured_depth, max_measured_depth)\n",
    "    if surface_points_combined is None:\n",
    "        surface_points_combined = surf_points_world[0, surf_mask[0]]\n",
    "    else:\n",
    "        surface_points_combined = torch.cat([surface_points_combined, surf_points_world[0, surf_mask[0]]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface_points_0 = surf_points_world[0, surf_mask[0]]\n",
    "# Debug\n",
    "surf_pc = o3d.geometry.PointCloud()\n",
    "surf_pc.points = o3d.utility.Vector3dVector(surface_points_combined)\n",
    "down_surf_pc = surf_pc\n",
    "# down_surf_pc = surf_pc.voxel_down_sample(voxel_size=scene_voxel_downsample_size) # 5mm\n",
    "# # If more than max points, uniformly sample\n",
    "# if len(down_surf_pc.points) > max_points:\n",
    "#     indices = np.random.choice(np.arange(len(down_surf_pc.points)), max_points, replace=False)\n",
    "#     down_surf_pc = down_surf_pc.select_by_index(indices)\n",
    "down_surf_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.0, 0.2, 1]), (np.asarray(down_surf_pc.points).shape[0], 1)))\n",
    "visualizer6 = JVisualizer()\n",
    "visualizer6.add_geometry(down_surf_pc)\n",
    "seen_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.0, 0.0, 0.0]), (np.asarray(seen_pc.points).shape[0], 1)))\n",
    "visualizer6.add_geometry(seen_pc)\n",
    "# visualizer6.add_geometry(pc_full)\n",
    "visualizer6.show()\n",
    "down_surf_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample grasps with GPG:\n",
    "\n",
    "# downsampl & crop the cloud:\n",
    "down_surf_pc = surf_pc.voxel_down_sample(voxel_size=scene_voxel_downsample_size) # 5mm\n",
    "down_surf_pc_cropped = down_surf_pc.crop(o3d.geometry.AxisAlignedBoundingBox(np.array([0.0, 0.0, 0.0]), np.array([size, size, size])))\n",
    "sampler = GpgGraspSamplerPcl(sim.gripper.finger_depth-0.0075) # Franka finger depth is actually a little less than 0.05\n",
    "safety_dist_above_table = sim.gripper.finger_depth # table is spawned at finger_depth\n",
    "# We also get the 'origin point' i.e. the point from the point cloud that was sampled that resulted in each gpg grasp\n",
    "grasps, grasps_pos, grasps_rot, origin_points = sampler.sample_grasps(down_surf_pc_cropped, num_grasps=15, max_num_samples=240,\n",
    "                                    safety_dis_above_table=safety_dist_above_table, show_final_grasps=False, return_origin_point=True)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Viz grasps\n",
    "seen_grasps_scene = trimesh.Scene()\n",
    "# unseen_grasps_scene = trimesh.Scene()\n",
    "from neugraspnet.utils import visual\n",
    "seen_grasp_mesh_list = [visual.grasp2mesh(g) for g in grasps]\n",
    "# unseen_grasp_mesh_list = [visual.grasp2mesh(g, color='red') for g in unseen_area_grasps]\n",
    "for i, g_mesh in enumerate(seen_grasp_mesh_list):\n",
    "    seen_grasps_scene.add_geometry(g_mesh, node_name=f'grasp_{i}')\n",
    "# for i, g_mesh in enumerate(unseen_grasp_mesh_list):\n",
    "#     unseen_grasps_scene.add_geometry(g_mesh, node_name=f'grasp_{i}')\n",
    "composed_scene = trimesh.Scene([scene_mesh, seen_grasps_scene])#, unseen_grasps_scene])\n",
    "composed_scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = composed_scene.camera\n",
    "camera_transform = camera.transform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GIGA-6DoF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "838152720d4d6971dc8654fbdb394087b2421f5e78d77e4d8c651076f26533d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
